{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc477a38-e020-453d-bd26-080133a73645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA model and dictionary loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Load the LDA model\n",
    "with open('../static/model/lda_model.pickle', 'rb') as file:\n",
    "    lda_model = pickle.load(file)\n",
    "\n",
    "# Load the dictionary\n",
    "with open('../static/model/id2word.pickle', 'rb') as file:\n",
    "    id2word = pickle.load(file)\n",
    "\n",
    "print(\"LDA model and dictionary loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c6cee92-970b-492c-b282-bd7afc7d121a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: use user create code need\n",
      "Topic 1: go get time make people\n",
      "Topic 2: work make time get need\n",
      "Topic 3: datum model use value number\n",
      "Topic 4: also market system year new\n"
     ]
    }
   ],
   "source": [
    "# Step 7: View topics with meaningful descriptions\n",
    "def get_topic_description(model, num_words=5):\n",
    "    topic_descriptions = {}\n",
    "    for idx, topic in model.print_topics(num_words=num_words):\n",
    "        # Extract the words from the topic\n",
    "        words = topic.split('\"')\n",
    "        # Get the meaningful words\n",
    "        meaningful_words = [words[i].strip() for i in range(1, len(words), 2)]\n",
    "        # Combine the words into a single string\n",
    "        topic_description = ' '.join(meaningful_words)\n",
    "        topic_descriptions[idx] = topic_description\n",
    "    return topic_descriptions\n",
    "\n",
    "# Get and print the topic descriptions\n",
    "topic_descriptions = get_topic_description(lda_model)\n",
    "for idx, description in topic_descriptions.items():\n",
    "    print(f\"Topic {idx}: {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4412ae91-d77f-4ffc-9312-c83e6d77e971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: use, user, create, code, need, datum, application, file, app, run\n",
      "Topic 1: go, get, time, make, people, know, say, life, feel, take\n",
      "Topic 2: work, make, time, get, need, people, good, help, team, well\n",
      "Topic 3: datum, model, use, value, number, time, image, feature, function, example\n",
      "Topic 4: also, market, system, year, new, people, make, country, company, many\n"
     ]
    }
   ],
   "source": [
    "# Define the number of top words to extract per topic\n",
    "TOP_WORDS = 10\n",
    "\n",
    "# Extract top words for each topic\n",
    "topics = lda_model.print_topics(num_topics=5, num_words=TOP_WORDS)\n",
    "topic_words = {}\n",
    "\n",
    "for topic in topics:\n",
    "    topic_id, topic_description = topic\n",
    "    # Extract words from the topic description\n",
    "    words = [word.split('*\"')[1].strip('\"') for word in topic_description.split(' + ')]\n",
    "    topic_words[topic_id] = words\n",
    "\n",
    "# Display the top words for each topic\n",
    "for topic_id, words in topic_words.items():\n",
    "    print(f\"Topic {topic_id}: {', '.join(words)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f1ce778-73a8-475f-bb2b-42173b028592",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chamuditha\\Desktop\\IRWA_project\\env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "C:\\Users\\Chamuditha\\Desktop\\IRWA_project\\env\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:649: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: use user create code need to be written in a way that is easy to understand.\n",
      "\n",
      "The following code snippet shows how to create a simple user\n",
      "Topic 1: go get time make people happy.\n",
      "\n",
      "\"I think it's a good thing that we're doing this,\" he said. \"We're not\n",
      "Topic 2: work make time get needlessly long.\n",
      "\n",
      "The problem is that the time it takes to get to the end of a sentence is often too long\n",
      "Topic 3: datum model use value number of the model.\n",
      "\n",
      "The model is then used to calculate the value of a given value. The model can be\n",
      "Topic 4: also market system year new)\n",
      "\n",
      "The following table lists the current market prices for the following products:\n",
      ".\n",
      " (1) The following is\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# Load GPT-2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "def generate_topic_description(topic_words):\n",
    "    # Prepare the input text for GPT-2\n",
    "    input_text = ' '.join(topic_words)  # Combine topic words into a single string\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')  # Encode the text\n",
    "\n",
    "    # Generate output\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        output = model.generate(input_ids, max_length=30, num_return_sequences=1, \n",
    "                                 no_repeat_ngram_size=2, early_stopping=True)\n",
    "\n",
    "    # Decode the generated output\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Step 7: View topics and generate descriptions\n",
    "def get_gpt2_topic_descriptions(model, tokenizer, lda_model, num_words=5):\n",
    "    topic_descriptions = {}\n",
    "    for idx, topic in lda_model.print_topics(num_words=num_words):\n",
    "        # Extract the words from the topic\n",
    "        words = topic.split('\"')\n",
    "        # Get the meaningful words\n",
    "        meaningful_words = [words[i].strip() for i in range(1, len(words), 2)]\n",
    "        # Generate a description using GPT-2\n",
    "        topic_description = generate_topic_description(meaningful_words)\n",
    "        topic_descriptions[idx] = topic_description\n",
    "    return topic_descriptions\n",
    "\n",
    "# Get and print the GPT-2 topic descriptions\n",
    "gpt2_topic_descriptions = get_gpt2_topic_descriptions(model, tokenizer, lda_model)\n",
    "for idx, description in gpt2_topic_descriptions.items():\n",
    "    print(f\"Topic {idx}: {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c59a543d-101a-4042-87f3-a91d3547197f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: use user create code need to be written in a way that is easy to understand.\n",
      "\n",
      "The following code snippet shows how to create a simple user\n",
      "Topic 1: go get time make people happy.\n",
      "\n",
      "\"I think it's a good thing that we're doing this,\" he said. \"We're not\n",
      "Topic 2: work make time get needlessly long.\n",
      "\n",
      "The problem is that the time it takes to get to the end of a sentence is often too long\n",
      "Topic 3: datum model use value number of the model.\n",
      "\n",
      "The model is then used to calculate the value of a given value. The model can be\n",
      "Topic 4: also market system year new)\n",
      "\n",
      "The following table lists the current market prices for the following products:\n",
      ".\n",
      " (1) The following is\n"
     ]
    }
   ],
   "source": [
    "def generate_topic_description(topic_words):\n",
    "    # Prepare the input text for GPT-2\n",
    "    input_text = ' '.join(topic_words)  # Combine topic words into a single string\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')  # Encode the text\n",
    "\n",
    "    # Generate output\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        output = model.generate(input_ids, max_length=30, num_return_sequences=1, \n",
    "                                 no_repeat_ngram_size=2, early_stopping=True)\n",
    "\n",
    "    # Decode the generated output\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Step 7: View topics and generate descriptions\n",
    "def get_gpt2_topic_descriptions(model, tokenizer, lda_model, num_words=5):\n",
    "    topic_descriptions = {}\n",
    "    for idx, topic in lda_model.print_topics(num_words=num_words):\n",
    "        # Extract the words from the topic\n",
    "        words = topic.split('\"')\n",
    "        # Get the meaningful words\n",
    "        meaningful_words = [words[i].strip() for i in range(1, len(words), 2)]\n",
    "        # Generate a description using GPT-2\n",
    "        topic_description = generate_topic_description(meaningful_words)\n",
    "        topic_descriptions[idx] = topic_description\n",
    "    return topic_descriptions\n",
    "\n",
    "# Get and print the GPT-2 topic descriptions\n",
    "gpt2_topic_descriptions = get_gpt2_topic_descriptions(model, tokenizer, lda_model)\n",
    "for idx, description in gpt2_topic_descriptions.items():\n",
    "    print(f\"Topic {idx}: {description}\")\n",
    "\n",
    "# Input the text and return the meaningful topics\n",
    "def get_meaningful_topics_from_input(text, lda_model, dictionary):\n",
    "    # Process the input text and get the topics\n",
    "    topics = process_input_text(text, lda_model, dictionary)\n",
    "    \n",
    "    # Generate GPT-2 topic descriptions\n",
    "    gpt2_topic_descriptions = get_gpt2_topic_descriptions(model, tokenizer, lda_model, topics)\n",
    "    \n",
    "    # Print or return the topics with descriptions\n",
    "    for idx, description in gpt2_topic_descriptions.items():\n",
    "        print(f\"Topic {idx}: {description}\")\n",
    "\n",
    "# Example usage\n",
    "input_text = \"Climate change is a pressing global issue that affects ecosystems, weather patterns, and human health. Governments and organizations worldwide are implementing policies to reduce carbon emissions and promote renewable energy sources. Public awareness and education on environmental conservation are crucial for fostering sustainable practices. Technological advancements in green energy and waste management are also playing a significant role in mitigating the impacts of climate change.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31044c3e-b3ba-4aec-b333-d4deb4bb2851",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "decoding to str: need a bytes-like object, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0mTraceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 65\u001b[0m\n\u001b[0;32m     58\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClimate change is a pressing global issue that affects ecosystems, weather patterns, and human health. Governments and organizations worldwide are implementing policies to reduce carbon emissions and promote renewable energy sources. Public awareness and education on environmental conservation are crucial for fostering sustainable practices. Technological advancements in green energy and waste management are also playing a significant role in mitigating the impacts of climate change.\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with actual user input\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Get the dictionary from your LDA model training process\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Assuming you have the 'dictionary' from the LDA model training\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# If not, create it again from the corpus you used.\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Generate and print GPT-2 topic descriptions for the input text\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m gpt2_topic_descriptions_input \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_topic_descriptions_from_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlda_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid2word\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, description \u001b[38;5;129;01min\u001b[39;00m gpt2_topic_descriptions_input\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTopic \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdescription\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 47\u001b[0m, in \u001b[0;36mgenerate_topic_descriptions_from_input\u001b[1;34m(input_text, lda_model, dictionary, model, tokenizer, num_words)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_topic_descriptions_from_input\u001b[39m(input_text, lda_model, dictionary, model, tokenizer, num_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m# Infer topics from the input text\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m     inferred_topics \u001b[38;5;241m=\u001b[39m \u001b[43minfer_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlda_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdictionary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_topics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_words\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     topic_descriptions \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, topic_words \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(inferred_topics):\n\u001b[0;32m     51\u001b[0m         \u001b[38;5;66;03m# Generate a description using GPT-2\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[15], line 30\u001b[0m, in \u001b[0;36minfer_topics\u001b[1;34m(input_text, lda_model, dictionary, num_topics)\u001b[0m\n\u001b[0;32m     27\u001b[0m processed_text \u001b[38;5;241m=\u001b[39m preprocess_text(input_text)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Convert text to bag-of-words\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m bow_vector \u001b[38;5;241m=\u001b[39m \u001b[43mdictionary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdoc2bow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Get the LDA model topics and top words\u001b[39;00m\n\u001b[0;32m     33\u001b[0m topics \u001b[38;5;241m=\u001b[39m lda_model\u001b[38;5;241m.\u001b[39mget_document_topics(bow_vector)\n",
      "File \u001b[1;32m~\\Desktop\\IRWA_project\\env\\Lib\\site-packages\\gensim\\corpora\\dictionary.py:246\u001b[0m, in \u001b[0;36mDictionary.doc2bow\u001b[1;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[0;32m    244\u001b[0m counter \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m document:\n\u001b[1;32m--> 246\u001b[0m     counter[w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(w, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    248\u001b[0m token2id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken2id\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_update \u001b[38;5;129;01mor\u001b[39;00m return_missing:\n",
      "\u001b[1;31mTypeError\u001b[0m: decoding to str: need a bytes-like object, list found"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import Phrases\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Preprocess the input text\n",
    "def preprocess_text(input_text):\n",
    "    # Tokenization and cleaning\n",
    "    tokens = simple_preprocess(input_text, deacc=True)  # Remove punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # You can add bigrams and trigrams if you did it during training\n",
    "    bigram = Phrases([tokens], min_count=5)\n",
    "    trigram = Phrases(bigram[[tokens]], min_count=5)\n",
    "    \n",
    "    # Form bigrams and trigrams\n",
    "    tokens = [bigram[tokens]]\n",
    "    tokens = [trigram[tokens]]\n",
    "    \n",
    "    return tokens[0]\n",
    "\n",
    "# Infer topics from the input text\n",
    "def infer_topics(input_text, lda_model, dictionary, num_topics=5):\n",
    "    # Preprocess the text\n",
    "    processed_text = preprocess_text(input_text)\n",
    "    \n",
    "    # Convert text to bag-of-words\n",
    "    bow_vector = dictionary.doc2bow(processed_text)\n",
    "    \n",
    "    # Get the LDA model topics and top words\n",
    "    topics = lda_model.get_document_topics(bow_vector)\n",
    "    \n",
    "    # Extract top topics and words\n",
    "    inferred_topics = []\n",
    "    for topic_num, prob in topics:\n",
    "        if prob > 0.1:  # Adjust the threshold based on relevance\n",
    "            topic_words = lda_model.show_topic(topic_num, topn=num_topics)\n",
    "            inferred_topics.append([word for word, _ in topic_words])\n",
    "    \n",
    "    return inferred_topics\n",
    "\n",
    "# Generate descriptions for the inferred topics using GPT-2\n",
    "def generate_topic_descriptions_from_input(input_text, lda_model, dictionary, model, tokenizer, num_words=5):\n",
    "    # Infer topics from the input text\n",
    "    inferred_topics = infer_topics(input_text, lda_model, dictionary, num_topics=num_words)\n",
    "    \n",
    "    topic_descriptions = {}\n",
    "    for idx, topic_words in enumerate(inferred_topics):\n",
    "        # Generate a description using GPT-2\n",
    "        topic_description = generate_topic_description(topic_words)\n",
    "        topic_descriptions[idx] = topic_description\n",
    "    \n",
    "    return topic_descriptions\n",
    "\n",
    "# Input text from the user\n",
    "input_text = \"Climate change is a pressing global issue that affects ecosystems, weather patterns, and human health. Governments and organizations worldwide are implementing policies to reduce carbon emissions and promote renewable energy sources. Public awareness and education on environmental conservation are crucial for fostering sustainable practices. Technological advancements in green energy and waste management are also playing a significant role in mitigating the impacts of climate change.\"  # Replace with actual user input\n",
    "\n",
    "# Get the dictionary from your LDA model training process\n",
    "# Assuming you have the 'dictionary' from the LDA model training\n",
    "# If not, create it again from the corpus you used.\n",
    "\n",
    "# Generate and print GPT-2 topic descriptions for the input text\n",
    "gpt2_topic_descriptions_input = generate_topic_descriptions_from_input(input_text, lda_model, id2word, model, tokenizer)\n",
    "for idx, description in gpt2_topic_descriptions_input.items():\n",
    "    print(f\"Topic {idx}: {description}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fbede14-9ecf-46db-ba07-cedb120c73b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: \"What is the meaning of the word 'what' in the context of a question?\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: \"What is the meaning of the word 'me'?\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 2: \"What is the meaning of the word 'good'?\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 3: \"Mean\"\n",
      "Topic 4: \"What is the meaning of the word 'market'?\"\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# Load GPT-2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, you can uncomment the following lines to use it\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "def generate_topic_name(top_words, max_length=50):\n",
    "    \"\"\"\n",
    "    Generates a topic name based on top words using GPT-2.\n",
    "    \"\"\"\n",
    "    prompt = f\"Topic with words {', '.join(top_words)}:\\nMeaningful Topic Name:\"\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')  # Add device=device if using GPU\n",
    "    outputs = model.generate(inputs, max_length=max_length, num_return_sequences=1, \n",
    "                             no_repeat_ngram_size=2, \n",
    "                             early_stopping=True)\n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract the generated topic name\n",
    "    topic_name = generated.split(\"Meaningful Topic Name:\")[1].strip().split('\\n')[0]\n",
    "    return topic_name\n",
    "\n",
    "# Generate topic names for all topics\n",
    "topic_names = {}\n",
    "\n",
    "for topic_id, words in topic_words.items():\n",
    "    name = generate_topic_name(words)\n",
    "    topic_names[topic_id] = name\n",
    "    print(f\"Topic {topic_id}: {name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8941f60b-caa0-4997-bb7b-245d40c5c26c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_words_lemmatized' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mphrases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Phraser\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Train the bigram model\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m bigram \u001b[38;5;241m=\u001b[39m Phrases(\u001b[43mclean_words_lemmatized\u001b[49m, min_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m      6\u001b[0m bigram_mod \u001b[38;5;241m=\u001b[39m Phraser(bigram)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Train the trigram model using the bigram-transformed corpus\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clean_words_lemmatized' is not defined"
     ]
    }
   ],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "# Train the bigram model\n",
    "bigram = Phrases(clean_words_lemmatized, min_count=5, threshold=50)\n",
    "bigram_mod = Phraser(bigram)\n",
    "\n",
    "# Train the trigram model using the bigram-transformed corpus\n",
    "trigram = Phrases(bigram[clean_words_lemmatized], threshold=50)\n",
    "trigram_mod = Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85a72a24-88b3-4763-88b1-99a9a7fc2da3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bigram_mod' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 65\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Assuming bigram_mod and trigram_mod are already defined as in your original code\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# If not, you need to load or define them here\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Example document\u001b[39;00m\n\u001b[0;32m     63\u001b[0m new_doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe latest advancements in machine learning algorithms have significantly improved data analysis and prediction accuracy.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 65\u001b[0m topic_id, topic_name \u001b[38;5;241m=\u001b[39m assign_topic(new_doc, lda_model, id2word, nlp, stop_words, \u001b[43mbigram_mod\u001b[49m, trigram_mod, topic_names)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssigned Topic ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopic_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Topic Name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopic_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bigram_mod' is not defined"
     ]
    }
   ],
   "source": [
    "def preprocess_new_document(document, nlp, stop_words, bigram_mod, trigram_mod):\n",
    "    \"\"\"\n",
    "    Preprocesses the new document to the format required by the LDA model.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    # Remove punctuations\n",
    "    clean_text = re.sub('[,\\\\.!?â€™]', '', document)\n",
    "    # Remove digits\n",
    "    clean_text = re.sub('\\\\w*\\\\d\\\\w*', '', clean_text)\n",
    "    # Convert to lowercase\n",
    "    clean_text = clean_text.lower()\n",
    "    \n",
    "    # Tokenize and remove stopwords\n",
    "    words = gensim.utils.simple_preprocess(clean_text, deacc=True)\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Form bigrams and trigrams\n",
    "    words = bigram_mod[words]\n",
    "    words = trigram_mod[words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    doc = nlp(\" \".join(words))\n",
    "    words = [token.lemma_ for token in doc if token.pos_ in ['NOUN', 'ADJ', 'VERB', 'ADV', 'PROPN']]\n",
    "    \n",
    "    return words\n",
    "\n",
    "def assign_topic(document, lda_model, id2word, nlp, stop_words, bigram_mod, trigram_mod, topic_names):\n",
    "    \"\"\"\n",
    "    Assigns the most probable topic to the document and returns the topic name.\n",
    "    \"\"\"\n",
    "    # Preprocess the document\n",
    "    words = preprocess_new_document(document, nlp, stop_words, bigram_mod, trigram_mod)\n",
    "    \n",
    "    # Convert to bag-of-words\n",
    "    bow = id2word.doc2bow(words)\n",
    "    \n",
    "    # Get topic distribution\n",
    "    topic_distribution = lda_model.get_document_topics(bow)\n",
    "    \n",
    "    # Find the topic with the highest probability\n",
    "    if topic_distribution:\n",
    "        top_topic = max(topic_distribution, key=lambda x: x[1])[0]\n",
    "        topic_name = topic_names.get(top_topic, f\"Topic {top_topic}\")\n",
    "        return top_topic, topic_name\n",
    "    else:\n",
    "        return None, \"No Topic Assigned\"\n",
    "\n",
    "# Example usage\n",
    "import spacy\n",
    "\n",
    "# Load spacy model\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Define stop words (ensure consistency with training)\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['has', 'been', 're', 'com', 'edu', 'use'])  # same as during training\n",
    "\n",
    "# Assuming bigram_mod and trigram_mod are already defined as in your original code\n",
    "# If not, you need to load or define them here\n",
    "\n",
    "# Example document\n",
    "new_doc = \"The latest advancements in machine learning algorithms have significantly improved data analysis and prediction accuracy.\"\n",
    "\n",
    "topic_id, topic_name = assign_topic(new_doc, lda_model, id2word, nlp, stop_words, bigram_mod, trigram_mod, topic_names)\n",
    "print(f\"Assigned Topic ID: {topic_id}, Topic Name: {topic_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87f37953-c75f-44a0-bbf7-79396508258d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA model and dictionary loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: use user create code need to be written in a way that is easy to understand.\n",
      "\n",
      "The following code snippet shows how to create a simple user\n",
      "Topic 1: go get time make people happy.\n",
      "\n",
      "\"I think it's a good thing that we're doing this,\" he said. \"We're not\n",
      "Topic 2: also market system year new)\n",
      "\n",
      "The following table lists the current market prices for the following products:\n",
      ".\n",
      " (1) The following is\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Phrases\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# Load the LDA model\n",
    "with open('../static/model/lda_model.pickle', 'rb') as file:\n",
    "    lda_model = pickle.load(file)\n",
    "\n",
    "# Load the dictionary\n",
    "with open('../static/model/id2word.pickle', 'rb') as file:\n",
    "    id2word = pickle.load(file)\n",
    "\n",
    "print(\"LDA model and dictionary loaded successfully.\")\n",
    "\n",
    "# Load GPT-2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Preprocess the input text\n",
    "def preprocess_text(input_text):\n",
    "    # Tokenization and cleaning\n",
    "    tokens = simple_preprocess(input_text, deacc=True)  # Remove punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # You can add bigrams and trigrams if you did it during training\n",
    "    bigram = Phrases([tokens], min_count=5)\n",
    "    trigram = Phrases(bigram[[tokens]], min_count=5)\n",
    "    \n",
    "    # Form bigrams and trigrams\n",
    "    tokens = bigram[tokens]\n",
    "    tokens = trigram[tokens]\n",
    "    \n",
    "    return tokens  # Return the flattened list of tokens\n",
    "\n",
    "# Infer topics from the input text\n",
    "def infer_topics(input_text, lda_model, dictionary, num_topics=5):\n",
    "    # Preprocess the text\n",
    "    processed_text = preprocess_text(input_text)\n",
    "    \n",
    "    # Filter out tokens that are not in the dictionary\n",
    "    processed_text = [word for word in processed_text if word in dictionary.token2id]\n",
    "\n",
    "    if len(processed_text) == 0:\n",
    "        raise ValueError(\"None of the tokens in the input text are present in the LDA dictionary.\")\n",
    "    \n",
    "    # Convert text to bag-of-words\n",
    "    bow_vector = dictionary.doc2bow(processed_text)\n",
    "    \n",
    "    # Get the LDA model topics and top words\n",
    "    topics = lda_model.get_document_topics(bow_vector)\n",
    "    \n",
    "    # Extract top topics and words\n",
    "    inferred_topics = []\n",
    "    for topic_num, prob in topics:\n",
    "        if prob > 0.1:  # Adjust the threshold based on relevance\n",
    "            topic_words = lda_model.show_topic(topic_num, topn=num_topics)\n",
    "            inferred_topics.append([word for word, _ in topic_words])\n",
    "    \n",
    "    return inferred_topics\n",
    "\n",
    "# Generate descriptive text from topic words using GPT-2\n",
    "def generate_topic_description(topic_words):\n",
    "    # Prepare the input text for GPT-2\n",
    "    input_text = ' '.join(topic_words)  # Combine topic words into a single string\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')  # Encode the text\n",
    "\n",
    "    # Generate output\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        output = model.generate(input_ids, max_length=30, num_return_sequences=1, \n",
    "                                 no_repeat_ngram_size=2, early_stopping=True)\n",
    "\n",
    "    # Decode the generated output\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Generate descriptions for inferred topics from input text\n",
    "def generate_topic_descriptions_from_input(input_text, lda_model, dictionary, model, tokenizer, num_words=5):\n",
    "    # Infer topics from the input text\n",
    "    inferred_topics = infer_topics(input_text, lda_model, dictionary, num_topics=num_words)\n",
    "    \n",
    "    topic_descriptions = {}\n",
    "    for idx, topic_words in enumerate(inferred_topics):\n",
    "        # Generate a description using GPT-2\n",
    "        topic_description = generate_topic_description(topic_words)\n",
    "        topic_descriptions[idx] = topic_description\n",
    "    \n",
    "    return topic_descriptions\n",
    "\n",
    "# Example usage: Input text from the user\n",
    "input_text = \"Climate change is a pressing global issue that affects ecosystems, weather patterns, and human health.\"  # Replace with actual user input\n",
    "\n",
    "# Generate and print GPT-2 topic descriptions for the input text\n",
    "gpt2_topic_descriptions_input = generate_topic_descriptions_from_input(input_text, lda_model, id2word, model, tokenizer)\n",
    "for idx, description in gpt2_topic_descriptions_input.items():\n",
    "    print(f\"Topic {idx}: {description}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc659230-66b8-4ac8-8a35-47dd9e0941d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
